{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-belle",
   "metadata": {},
   "source": [
    "# Mission 1\n",
    "\n",
    "주어진 바닐라 데이터를 가지고 이미지와 해당하는 클래스 Label (18개의 클래스 중 하나)을 생성할 수 있는 Pytorch **Dataset** Class를 직접 생성해보세요.\n",
    "\n",
    "18개의 클래스를 만드셨다면, 그 타겟 클래스의 분포도 다시 한번 확인해보면 좋겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exact-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 path 저장\n",
    "\n",
    "img_dir = glob.glob('/opt/ml/input/data/train/images/*')\n",
    "train_images_path = []\n",
    "\n",
    "for path in img_dir:\n",
    "    img_path = glob.glob(os.path.join(path, '*'))\n",
    "    train_images_path.extend(sorted(img_path))\n",
    "len(train_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = train_images_path[0]\n",
    "info = path.split('/')[-2].split('_')\n",
    "gender, age = info[1], int(info[3])\n",
    "label = 0\n",
    "if gender == 'female':\n",
    "    label += 3\n",
    "\n",
    "if 30 <= age and age < 60:\n",
    "    label += 1\n",
    "elif 60 <= age:\n",
    "    label += 2\n",
    "    \n",
    "info = path.split('/')[-1]\n",
    "if 'incorrect' in info:\n",
    "    label += 6\n",
    "elif 'normal' in info:\n",
    "    label += 12\n",
    "    \n",
    "print(gender, age, info)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-supplement",
   "metadata": {},
   "source": [
    "## Dataset Class 생성\n",
    "\n",
    "### Map-style datasets\n",
    "https://pytorch.org/docs/stable/data.html#map-style-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/opt/ml/input/data/train/images/000523_female_Asian_51/incorrect_mask.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_images_path, transform=None):\n",
    "        self.train_images_path = train_images_path\n",
    "        self.size = len(self.train_images_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        input : index\n",
    "        output: (image tenor(width x heigth x channel), label)\n",
    "        \"\"\"\n",
    "\n",
    "        path = self.train_images_path[index]\n",
    "        img = cv2.imread(img_path)  # shape (height x width x channel) = (512 x 384 x 3)\n",
    "        label = self.get_label(path)\n",
    "        sample = {'image': img, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 고정해서 출력하는 것이 좋을까? - self.size\n",
    "        # 그 떄 그 때 측정하는 것이 좋을까? 혹시 누가 삭제하면,, ㅜㅜ - len(self.train_images_path)\n",
    "        return self.size\n",
    "    \n",
    "    def get_label(self, path):\n",
    "        \"\"\"\n",
    "        input : path\n",
    "            ex. '/opt/ml/input/data/train/images/000523_female_Asian_51/incorrect_mask.jpg'\n",
    "        output: label\n",
    "        \"\"\"\n",
    "        info = path.split('/')[-2].split('_')\n",
    "        gender, age = info[1], int(info[3])\n",
    "        label = 0\n",
    "        if gender == 'female':\n",
    "            label += 3\n",
    "\n",
    "        if 30 <= age and age < 60:\n",
    "            label += 1\n",
    "        elif 60 <= age:\n",
    "            label += 2\n",
    "\n",
    "        info = path.split('/')[-1]\n",
    "        if 'incorrect' in info:\n",
    "            label += 6\n",
    "        elif 'normal' in info:\n",
    "            label += 12\n",
    "    \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(train_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set[10]\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(train_images_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-outreach",
   "metadata": {},
   "source": [
    "## 타겟 클래스의 분포확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiac-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 path 저장\n",
    "\n",
    "img_dir = glob.glob('/opt/ml/input/data/train/images/*')\n",
    "\n",
    "class_num = 18\n",
    "classes = [[] for _ in range(class_num)]\n",
    "\n",
    "for path in img_dir:\n",
    "    # gender와 age에 따라 class 설정\n",
    "    info = path.split('/')[-1].split('_')\n",
    "    gender, age = info[1], int(info[3])\n",
    "    c = 0\n",
    "    if gender == 'female':\n",
    "        c += 3\n",
    "        \n",
    "    if 30 <= age and age < 60:\n",
    "        c += 1\n",
    "    elif 60 <= age:\n",
    "        c += 2\n",
    "    \n",
    "    new_c = c\n",
    "    img_paths = glob.glob(os.path.join(path, '*'))\n",
    "    for img_path in img_paths:\n",
    "        c = new_c\n",
    "        if 'incorrect' in img_path:\n",
    "            c += 6\n",
    "        elif 'normal' in img_path:\n",
    "            c += 12\n",
    "        classes[c].append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 18\n",
    "index = []\n",
    "count = []\n",
    "colors = ['#FEA443', '#F3FEB0', '#F2EDD0', '#F2D479', \n",
    "          '#C3B2AF', '#C2DDC8', '#BDCC94', '#BCBF50', \n",
    "          '#B0BAC3', '#AAB0B5', '#A9B6CC', '#A8C0CE', \n",
    "          '#A5AAA3', '#99BFB3', '#768591', '#705E78', \n",
    "          '#55967e', '#263959']\n",
    "for i in range(class_num):\n",
    "    index.append(i)\n",
    "    count.append(len(classes[i]))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(index, count, color=colors)\n",
    "plt.title('Class Group Exploration', fontsize=20)\n",
    "plt.xlabel('class', fontsize=18)\n",
    "plt.ylabel('count', fontsize=18)\n",
    "plt.xticks(index, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-criterion",
   "metadata": {},
   "source": [
    "# Mission 2\n",
    "\n",
    "강의때 보여드렸던 **torchvision**에 내장된 여러 Augmentation 함수와 **albumentation** 라이브러리의 여러 transform 기법을 적용해보세요. 적용해 보신 뒤에 실제로 어떻게 변환되어 나오는지 확인해보세요. 아마 **plot**형태로 그려서 확인해야 할거에요.\n",
    "\n",
    "그리고 이러한 Transforms를  추가한 Dataset이 과연 어느 정도의성능을 가지는지 체크해보세요. 혹여 너무 무거운 프로세스라면 생각보다 느리게 동작하겠죠? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tutorials.pytorch.kr/recipes/recipes/custom_dataset_transforms_loader.html\n",
    "class ToTensor(object):\n",
    "    \"\"\" 샘플 안에 있는 n차원 배열을 Tensor로 변홥힙니다. \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # 색깔 축들을 바꿔치기해야하는데 그 이유는 numpy와 torch의 이미지 표현방식이 다르기 때문입니다.\n",
    "        # numpy 이미지: H x W x C\n",
    "        # torch 이미지: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path,\n",
    "                                 transform=transforms.Compose([\n",
    "                                     ToTensor()\n",
    "                                 ]))\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "    \n",
    "    print(i, sample['image'].size, sample['label'])\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-double",
   "metadata": {},
   "source": [
    "### transforms - ToTensor()\n",
    "→ ToTensor 했을 때 성능이 더 좋다. 또 연산을 위해서는 ToTensor로 shape를 바꿔줘야한다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path,\n",
    "                                 transform=transforms.Compose([\n",
    "                                     ToTensor()\n",
    "                                 ]))\n",
    "dataloader = DataLoader(transformed_dataset)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-major",
   "metadata": {},
   "source": [
    "### DataLoader - num_workers\n",
    "\n",
    "→ num_workers=1 일 때 성능이 가장 좋다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=3, num_workers=1)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=3, num_workers=2)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=3, num_workers=3)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=3, num_workers=10)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=3, num_workers=100)\n",
    "\n",
    "for i, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    data, target = sample_batched['image'], sample_batched['label']\n",
    "    print(i, data.size(), target)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-victoria",
   "metadata": {},
   "source": [
    "### torchvision - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms.Compose([\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_images_path, transform=None):\n",
    "        self.train_images_path = train_images_path\n",
    "        self.size = len(self.train_images_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        input : index\n",
    "        output: (image tenor(width x heigth x channel), label)\n",
    "        \"\"\"\n",
    "            \n",
    "        path = self.train_images_path[index]\n",
    "        label = self.get_label(path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = Image.open(path)  # PIL Image\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        else:\n",
    "            img = cv2.imread(path)  # type: numpy.ndarray, dtype: unit8, H x W x C\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        return (img, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 고정해서 출력하는 것이 좋을까? - self.size\n",
    "        # 그 떄 그 때 측정하는 것이 좋을까? 혹시 누가 삭제하면,, ㅜㅜ - len(self.train_images_path)\n",
    "        return self.size\n",
    "    \n",
    "    def get_label(self, path):\n",
    "        \"\"\"\n",
    "        input : path\n",
    "            ex. '/opt/ml/input/data/train/images/000523_female_Asian_51/incorrect_mask.jpg'\n",
    "        output: label\n",
    "        \"\"\"\n",
    "        info = path.split('/')[-2].split('_')\n",
    "        gender, age = info[1], int(info[3])\n",
    "        label = 0\n",
    "        if gender == 'female':\n",
    "            label += 3\n",
    "\n",
    "        if 30 <= age and age < 60:\n",
    "            label += 1\n",
    "        elif 60 <= age:\n",
    "            label += 2\n",
    "\n",
    "        info = path.split('/')[-1]\n",
    "        if 'incorrect' in info:\n",
    "            label += 6\n",
    "        elif 'normal' in info:\n",
    "            label += 12\n",
    "    \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = MyDataset(train_images_path,\n",
    "                                transform=transforms.Compose([\n",
    "                                     transforms.CenterCrop(280),\n",
    "                                     transforms.ToTensor(),\n",
    "                                 ]))\n",
    "dataloader = DataLoader(transformed_dataset)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "for i, sample_batched in enumerate(dataloader):\n",
    "    pix, label = sample_batched\n",
    "\n",
    "    img = np.squeeze(pix.numpy())  # C X H X W\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    ax = fig.add_subplot(5, 7, i+1)\n",
    "    ax.imshow(img) # H x W x C\n",
    "    ax.set_title(label[0])\n",
    "\n",
    "    if i==34:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_images_path, transform=None):\n",
    "        self.train_images_path = train_images_path\n",
    "        self.size = len(self.train_images_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        input : index\n",
    "        output: (image tenor(width x heigth x channel), label)\n",
    "        \"\"\"\n",
    "            \n",
    "        path = self.train_images_path[index]\n",
    "        label = self.get_label(path)\n",
    "        img = cv2.imread(path)  # type: numpy.ndarray, dtype: unit8, H x W x C\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)\n",
    "        \n",
    "        return (img, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 고정해서 출력하는 것이 좋을까? - self.size\n",
    "        # 그 떄 그 때 측정하는 것이 좋을까? 혹시 누가 삭제하면,, ㅜㅜ - len(self.train_images_path)\n",
    "        return self.size\n",
    "    \n",
    "    def get_label(self, path):\n",
    "        \"\"\"\n",
    "        input : path\n",
    "            ex. '/opt/ml/input/data/train/images/000523_female_Asian_51/incorrect_mask.jpg'\n",
    "        output: label\n",
    "        \"\"\"\n",
    "        info = path.split('/')[-2].split('_')\n",
    "        gender, age = info[1], int(info[3])\n",
    "        label = 0\n",
    "        if gender == 'female':\n",
    "            label += 3\n",
    "\n",
    "        if 30 <= age and age < 60:\n",
    "            label += 1\n",
    "        elif 60 <= age:\n",
    "            label += 2\n",
    "\n",
    "        info = path.split('/')[-1]\n",
    "        if 'incorrect' in info:\n",
    "            label += 6\n",
    "        elif 'normal' in info:\n",
    "            label += 12\n",
    "    \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=384, height=512),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.RandomBrightnessContrast(p=1),\n",
    "])\n",
    "\n",
    "transformed_dataset = MyDataset(train_images_path,\n",
    "                                transform = transform)\n",
    "dataloader = DataLoader(transformed_dataset)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "for i, sample_batched in enumerate(dataloader):\n",
    "    pix, label = sample_batched\n",
    "    pix = pix['image']\n",
    "    \n",
    "    img = np.squeeze(pix.numpy())  # H x W x C\n",
    "\n",
    "    ax = fig.add_subplot(5, 7, i+1)\n",
    "    ax.imshow(img) # H x W x C\n",
    "    ax.set_title(label[0])\n",
    "\n",
    "    if i==34:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-sequence",
   "metadata": {},
   "source": [
    "### 어떤 방법으로 augmentation 해주는 것이 좋을까..\n",
    "\n",
    "1. crop\n",
    "2. 좌우 반전\n",
    "3. cutmix\n",
    "    - 음 다른 연령대랑 cutmix하는 건 음 괜찮은가?\n",
    "    - 마스크 쓴거랑 안쓴거 cutmix로 하면 incorrect라고 해야하는 건가?\n",
    "4. 밝기 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-wisdom",
   "metadata": {},
   "source": [
    "**Albumentations**\n",
    "<br/>https://github.com/albumentations-team/albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# Declare an augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256),\n",
    "    A.ShiftScaleRotate(p=1),\n",
    "    A.RandomBrightnessContrast(p=2),\n",
    "])\n",
    "\n",
    "# Read an image with OpenCV and conver it to the RGB colorspace\n",
    "image = cv2.imread('/opt/ml/input/data/train/images/000523_female_Asian_51/incorrect_mask.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Augment an image\n",
    "transformed = transform(image=image)\n",
    "transformed_image = transformed['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.imshow(image) # H x W x C\n",
    "ax1.set_title('original imgae')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.imshow(transformed_image)\n",
    "ax2.set_title('transformed image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-testimony",
   "metadata": {},
   "source": [
    "흠.. 45도 회전(ShiftScaleRotate).. 할까 말까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-market",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
